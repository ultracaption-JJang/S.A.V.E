{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCKk0QIZA3Nb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l703629gPc2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/michelecafagna26/CLIPCap.git\n",
            "  Cloning https://github.com/michelecafagna26/CLIPCap.git to /private/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/pip-req-build-5a_7e3o1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/michelecafagna26/CLIPCap.git /private/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/pip-req-build-5a_7e3o1\n",
            "  Resolved https://github.com/michelecafagna26/CLIPCap.git to commit 545c5574fee8e3a56dec3e5c3f86388269a06f1d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting clip@ git+https://github.com/openai/CLIP@main (from clipcap==1.0)\n",
            "  Cloning https://github.com/openai/CLIP (to revision main) to /private/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/pip-install-ri62hat6/clip_dbaadd16ea194a03a22e082ff52f152c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP /private/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/pip-install-ri62hat6/clip_dbaadd16ea194a03a22e082ff52f152c\n",
            "  Resolved https://github.com/openai/CLIP to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==4.16.0 (from clipcap==1.0)\n",
            "  Downloading transformers-4.16.0-py3-none-any.whl.metadata (61 kB)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from clipcap==1.0) (4.66.5)\n",
            "Requirement already satisfied: regex in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from clipcap==1.0) (2024.9.11)\n",
            "Requirement already satisfied: ftfy in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from clipcap==1.0) (6.3.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from transformers==4.16.0->clipcap==1.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from transformers==4.16.0->clipcap==1.0) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from transformers==4.16.0->clipcap==1.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from transformers==4.16.0->clipcap==1.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from transformers==4.16.0->clipcap==1.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from transformers==4.16.0->clipcap==1.0) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.16.0->clipcap==1.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from transformers==4.16.0->clipcap==1.0) (0.20.1)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0) (2.5.0)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0) (0.20.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from ftfy->clipcap==1.0) (0.2.13)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0->clipcap==1.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0->clipcap==1.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from requests->transformers==4.16.0->clipcap==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from requests->transformers==4.16.0->clipcap==1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from requests->transformers==4.16.0->clipcap==1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from requests->transformers==4.16.0->clipcap==1.0) (2024.8.30)\n",
            "Requirement already satisfied: click in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from sacremoses->transformers==4.16.0->clipcap==1.0) (8.1.7)\n",
            "Collecting joblib (from sacremoses->transformers==4.16.0->clipcap==1.0)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from torch->clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from torch->clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0) (3.1.4)\n",
            "Collecting sympy==1.13.1 (from torch->clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from sympy==1.13.1->torch->clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from torchvision->clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/NIPA/lib/python3.11/site-packages (from jinja2->torch->clip@ git+https://github.com/openai/CLIP@main->clipcap==1.0) (2.1.3)\n",
            "Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Building wheels for collected packages: clipcap, clip\n",
            "  Building wheel for clipcap (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for clipcap: filename=clipcap-1.0-py3-none-any.whl size=8590 sha256=8327880da35a7417fd9b8874121784546337b85951c4ae37c2bf38ee395bd787\n",
            "  Stored in directory: /private/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/pip-ephem-wheel-cache-2alf7h9s/wheels/11/65/af/45ca4b4b1ee1d7d9f6e4c53682358a1518b9846f8908407bd2\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=664d8d5baf72bf233b5648c689630551d55006a58a69a391e13e0f471138287a\n",
            "  Stored in directory: /private/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/pip-ephem-wheel-cache-2alf7h9s/wheels/d1/b1/83/a2ab8db4aa2fdb8af128767d03e3af19e486ac701bb9b56054\n",
            "Successfully built clipcap clip\n",
            "Installing collected packages: sympy, joblib, sacremoses, transformers, clip, clipcap\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.2\n",
            "    Uninstalling sympy-1.13.2:\n",
            "      Successfully uninstalled sympy-1.13.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.45.2\n",
            "    Uninstalling transformers-4.45.2:\n",
            "      Successfully uninstalled transformers-4.45.2\n",
            "Successfully installed clip-1.0 clipcap-1.0 joblib-1.4.2 sacremoses-0.1.1 sympy-1.13.1 transformers-4.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/michelecafagna26/CLIPCap.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "git clone https://github.com/rmokady/CLIP_prefix_caption && cd CLIP_prefix_caption\n",
        "conda env create -f environment.yml\n",
        "conda activate clip_prefix_caption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 데이터셋을 pkl 형식으로 만들어야 합니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/MLB/NIPA/CLIP_prefix_caption/parse_coco.py --clip_model_type ViT-B/32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TRAIN\n",
        "\n",
        "공식 문서 참고해서 파인튜닝 할 것\n",
        "\n",
        "\n",
        "epochs = 10 \n",
        "필요시 train.py에서 바꿀것 \n",
        "batch_size = 40 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /content/drive/MyDrive/MLB/NIPA/CLIP_prefix_caption/train.py --data /content/drive/MyDrive/MLB/NIPA/CLIP_prefix_caption_ViT-B_32_train.pkl --out_dir /content/drive/MyDrive/MLB/NIPA/CLIP_prefix_caption_ViT-B_32_train_out --pretrained_model /content/drive/MyDrive/MLB/NIPA/CLIP_prefix_caption/model_wieghts.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "from clipcap import ClipCaptionModel\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        ")\n",
        "import torch\n",
        "import clip\n",
        "import requests\n",
        "from PIL import Image\n",
        "import os \n",
        "import json "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load pretrained model\n",
        "def load_pretrained_model(model, pretrained_path):\n",
        "    if os.path.isfile(pretrained_path):\n",
        "        print(f\"Loading pre-trained model from {pretrained_path}\")\n",
        "        # strict=False를 추가하여 일부 매개변수가 없어도 로딩 가능하도록 설정\n",
        "        model.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')), strict=False)\n",
        "        print(\"Model loaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Pre-trained model file {pretrained_path} not found.\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pretrained model by openAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/NIPA/lib/python3.11/site-packages/transformers/modeling_utils.py:1435: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained model from /Users/psjj/Downloads/model_wieghts.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/ipykernel_98814/4285155122.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')), strict=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "Generated caption: a couple of cows are laying in a field together \n",
            " \n",
            " \n",
            "                                                   \n"
          ]
        }
      ],
      "source": [
        "# load pretrained model\n",
        "def load_pretrained_model(model, pretrained_path):\n",
        "    if os.path.isfile(pretrained_path):\n",
        "        print(f\"Loading pre-trained model from {pretrained_path}\")\n",
        "        # strict=False를 추가하여 일부 매개변수가 없어도 로딩 가능하도록 설정\n",
        "        model.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')), strict=False)\n",
        "        print(\"Model loaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Pre-trained model file {pretrained_path} not found.\")\n",
        "    return model\n",
        "\n",
        "#model_path = \"/Users/psjj/Downloads/coco_prefix-009.pt\"  # change accordingly\n",
        "model_path = '/Users/psjj/Downloads/model_wieghts.pt'\n",
        "# load clip\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "prefix_length = 10\n",
        "\n",
        "# load ClipCap\n",
        "model = ClipCaptionModel(prefix_length, tokenizer=tokenizer)\n",
        "\n",
        "# load the pretrained model weights\n",
        "model = load_pretrained_model(model, model_path)\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# load the image\n",
        "image_dir = '/Users/psjj/Downloads/coco2017/test2017/000000000019.jpg'\n",
        "raw_image = Image.open(image_dir).convert('RGB')\n",
        "\n",
        "# extract the prefix\n",
        "image = preprocess(raw_image).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "\n",
        "# assuming generate_beam() is correctly implemented in ClipCaptionModel\n",
        "caption = model.generate_beam(embed=prefix_embed)[0]\n",
        "\n",
        "print(\"Generated caption:\", caption)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained model from /Users/psjj/Downloads/coco_prefix-009.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/xz/3gsy8h1n2mq8_tmb5ll9f2j80000gn/T/ipykernel_98814/3215760643.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')), strict=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully.\n",
            "Generated caption: \n",
            "\n",
            "The following is a list of the most common, and the most common, and the most common, and the most common, and the most common, and the most common, and the most common, and the most common, and the most common, and the most common, and the most common, and the most common,\n"
          ]
        }
      ],
      "source": [
        "# load pretrained model\n",
        "def load_pretrained_model(model, pretrained_path):\n",
        "    if os.path.isfile(pretrained_path):\n",
        "        print(f\"Loading pre-trained model from {pretrained_path}\")\n",
        "        # strict=False를 추가하여 일부 매개변수가 없어도 로딩 가능하도록 설정\n",
        "        model.load_state_dict(torch.load(pretrained_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')), strict=False)\n",
        "        print(\"Model loaded successfully.\")\n",
        "    else:\n",
        "        print(f\"Pre-trained model file {pretrained_path} not found.\")\n",
        "    return model\n",
        "\n",
        "model_path = \"/Users/psjj/Downloads/coco_prefix-009.pt\"  # change accordingly\n",
        "#model_path = '/Users/psjj/Downloads/model_wieghts.pt'\n",
        "# load clip\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "prefix_length = 10\n",
        "\n",
        "# load ClipCap\n",
        "model = ClipCaptionModel(prefix_length, tokenizer=tokenizer)\n",
        "\n",
        "# load the pretrained model weights\n",
        "model = load_pretrained_model(model, model_path)\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# load the image\n",
        "image_dir = '/Users/psjj/Downloads/coco2017/test2017/000000000019.jpg' # your_path\n",
        "raw_image = Image.open(image_dir).convert('RGB')\n",
        "\n",
        "# extract the prefix\n",
        "image = preprocess(raw_image).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
        "    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "\n",
        "# assuming generate_beam() is correctly implemented in ClipCaptionModel\n",
        "caption = model.generate_beam(embed=prefix_embed)[0]\n",
        "\n",
        "print(\"Generated caption:\", caption)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
