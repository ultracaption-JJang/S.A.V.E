{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jCKk0QIZA3Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/michelecafagna26/CLIPCap.git\n"
      ],
      "metadata": {
        "id": "l703629gPc2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGgGaypjApvi"
      },
      "outputs": [],
      "source": [
        "from clipcap import ClipCaptionModel\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        ")\n",
        "import torch\n",
        "import clip\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/MLB/NIPA/clipcap-base-captioning-ft-hl-actions/pytorch_model.pt\" # change accordingly\n",
        "\n",
        "# load clip\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "prefix_length = 10\n",
        "\n",
        "# load ClipCap\n",
        "model = ClipCaptionModel(prefix_length, tokenizer=tokenizer)\n",
        "model.from_pretrained(model_path)\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# load the image\n",
        "#img_url = 'http://images.cocodataset.org/val2017/000000039764.jpg'\n",
        "image_dir = '/content/drive/MyDrive/MLB/NIPA/val_image/000000000285.jpg'\n",
        "\n",
        "#raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "\n",
        "raw_image = Image.open(image_dir).convert('RGB')\n",
        "\n",
        "# extract the prefix\n",
        "image = preprocess(raw_image).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    prefix = clip_model.encode_image(image).to(\n",
        "        device, dtype=torch.float32\n",
        "    )\n",
        "    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
        "\n",
        "# generate the caption\n",
        "model.generate_beam(embed=prefix_embed)[0]\n",
        "\n",
        "\n",
        "# >> \"she is posing for a photo.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/MLB/NIPA/clipcap-base-captioning-ft-hl-actions"
      ],
      "metadata": {
        "id": "mDDRSgLsCpKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@huggingface.co/michelecafagna26/clipcap-base-captioning-ft-hl-actions"
      ],
      "metadata": {
        "id": "rMoCD6M4A3Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface-cli login"
      ],
      "metadata": {
        "id": "vl_jK1JFDTwx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}