{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-30T10:56:59.444091Z","iopub.status.busy":"2024-10-30T10:56:59.443754Z","iopub.status.idle":"2024-10-30T10:57:20.372547Z","shell.execute_reply":"2024-10-30T10:57:20.371546Z","shell.execute_reply.started":"2024-10-30T10:56:59.444057Z"},"trusted":true},"outputs":[],"source":["import os\n","import json\n","import torch\n","import wandb\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import CLIPProcessor, CLIPModel, Trainer, TrainingArguments\n","from PIL import Image\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 1. Custom Dataset 준비 (JSON 파일에서 이미지와 텍스트 정보 로드)\n","class CustomCLIPDataset(Dataset):\n","    def __init__(self, json_path, image_dir, processor):\n","        with open(json_path, 'r') as f:\n","            self.data = json.load(f)  # JSON 파일 로드\n","        self.processor = processor\n","        self.image_dir = image_dir\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        image_id = str(item['image_id']).zfill(12)\n","        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n","        caption = item['caption']\n","        \n","        # 이미지 로드 및 전처리\n","        image = Image.open(image_path).convert(\"RGB\")\n","        inputs = self.processor(images=image, text=[caption], return_tensors=\"pt\", padding=True)\n","        \n","        return {\n","            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n","            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n","            \"pixel_values\": inputs[\"pixel_values\"].squeeze()\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n","    pixel_values = [item['pixel_values'].squeeze(0) for item in batch]\n","    \n","    input_ids_padded = pad_sequence(input_ids, batch_first=True)\n","    pixel_values = torch.stack(pixel_values)  \n","    \n","    return {\n","        'input_ids': input_ids_padded,\n","        'pixel_values': pixel_values\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # 커스텀 Trainer 클래스 정의\n","# class CLIPTrainer(Trainer):\n","#     def compute_loss(self, model, inputs, return_outputs=False):\n","#         # 모델에서 출력 얻기\n","#         outputs = model(input_ids=inputs[\"input_ids\"], \n","#                         attention_mask=inputs[\"attention_mask\"], \n","#                         pixel_values=inputs[\"pixel_values\"])\n","        \n","#         # 텍스트 및 이미지 임베딩 추출\n","#         image_features = outputs.image_embeds\n","#         text_features = outputs.text_embeds\n","        \n","#         # Contrastive Loss 계산\n","#         loss_fn = ContrastiveLoss()\n","#         loss = loss_fn(image_features, text_features)\n","        \n","#         return (loss, outputs) if return_outputs else loss"]},{"cell_type":"markdown","metadata":{},"source":["## 캐글: Add-ons에서 본인 wandb키 입력하기 \n","## 로컬: 알아서 ㅎㅎ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value = user_secrets.get_secret(\"wandbkey\")\n","os.environ[\"WANDB_API_KEY\"] = secret_value\n","\n","wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["## 파라미터"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sweep_config = {\n","    'method': 'random',  # 하이퍼파라미터 검색 방법 ('grid', 'random', 'bayes' 중 선택)\n","    'metric': {\n","        'name': 'loss',  # 최적화할 메트릭 이름\n","        'goal': 'minimize'  # 목표: 'maximize' 또는 'minimize'\n","    },\n","    'parameters': {\n","        'batch_size': {\n","            'values': [16, 32, 64]  # 실험할 배치 크기 값 목록\n","        },\n","        'learning_rate': {\n","            'distribution': 'uniform',  # 'uniform' 분포에서 값을 샘플링\n","            'min': 0.0001,\n","            'max': 0.001\n","        },\n","        'epochs': {\n","            'values': [10, 20, 30]  # 실험할 epoch 값 목록\n","        }\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_id = wandb.sweep(sweep_config, project=\"clip_experiment\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 데이터로더 부분 경로 바꿔줘야합니다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_image_dir = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_train\"\n","val_image_dir = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_val\"\n","train_json_file = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_train_captions.json\"\n","val_json_file = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_val_captions.json\"\n","\n","# 3. Initialize Dataset and DataLoader\n","train_dataset = CustomCLIPDataset(train_json_file, train_image_dir, processor)\n","val_dataset = CustomCLIPDataset(val_json_file, val_image_dir, processor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## TRAIN!!"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def train():\n","    wandb.init()  # Initialize W&B run\n","    batch_size = wandb.config.batch_size\n","    learning_rate = wandb.config.learning_rate\n","    epochs = wandb.config.epochs\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","    optimizer = Adam(model.parameters(), lr=learning_rate)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    model.to(device)\n","\n","    best_val_accuracy = 0.0  # Initialize the best validation accuracy\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        \n","        for batch in train_dataloader:\n","            optimizer.zero_grad()\n","\n","            input_ids = batch['input_ids'].to(device)\n","            pixel_values = batch['pixel_values'].to(device)\n","            \n","            outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n","            logits_per_image = outputs.logits_per_image\n","            labels = torch.arange(logits_per_image.size(0)).to(logits_per_image.device)\n","\n","            # Compute loss\n","            loss = loss_fn(logits_per_image, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_train_loss}\")\n","\n","        # Validation Loop\n","        model.eval()\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            for batch in val_dataloader:\n","                input_ids = batch['input_ids'].to(device)\n","                pixel_values = batch['pixel_values'].to(device)\n","\n","                outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n","                logits_per_image = outputs.logits_per_image\n","                predictions = torch.argmax(logits_per_image, dim=1)\n","                labels = torch.arange(predictions.size(0)).to(predictions.device)\n","\n","                correct += (predictions == labels).sum().item()\n","                total += labels.size(0)\n","\n","        val_accuracy = correct / total\n","        print(f\"Validation Accuracy: {val_accuracy}\")\n","\n","        # Log metrics to W&B\n","        wandb.log({\"epoch\": epoch, \"loss\": avg_train_loss, \"val_accuracy\": val_accuracy})\n","\n","        # Check if this is the best model so far\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            # Save the best model\n","            save_path = \"best_model.pth\"\n","            torch.save(model.state_dict(), save_path)\n","            print(f\"Best model saved with accuracy: {best_val_accuracy}\")\n","\n","            # Save model to W&B\n","            artifact = wandb.Artifact('best_model', type='model')\n","            artifact.add_file(save_path)\n","            wandb.log_artifact(artifact)\n","\n","# Start the W&B sweep agent\n","wandb.agent(sweep_id, train)"]},{"cell_type":"markdown","metadata":{},"source":["## 코사인 유사도 비교"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trained_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","trained_model.load_state_dict(torch.load(\"best_model.pth\"))\n","trained_model.to(device)\n","trained_model.eval()\n","\n","original_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","original_model.to(device)\n","original_model.eval()\n","\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 원하는 텍스트 데이터 입력\n","- 1.\t“A black SUV parked next to a ‘Pay Here’ parking meter on a busy street.”\n","- 2.\t“Cars lined up along the street near a parking sign, with a view of distant hills.”\n","- 3.\t“A parking area with vehicles and a visible parking payment station in the foreground.”\n","- 4.\t“An urban street scene with parked cars and a ‘Pay Here’ sign for parking fees.”\n","- 5.\t“A black vehicle with a license plate parked beside a meter that says ‘Pay Here’.”"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_path = \"/Users/psjj/Downloads/coco2017/realrealreal_coco_dataset/realreal_test/000000092212.jpg\"  # 테스트할 이미지 경로\n","text = \"Your descriptive text here\" \n","\n","image = Image.open(image_path).convert(\"RGB\")\n","inputs = processor(images=image, text=[text], return_tensors=\"pt\", padding=True)\n","pixel_values = inputs[\"pixel_values\"].to(device)\n","input_ids = inputs[\"input_ids\"].to(device)\n","attention_mask = inputs[\"attention_mask\"].to(device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    # 학습된 모델 임베딩\n","    trained_outputs = trained_model(input_ids=input_ids, pixel_values=pixel_values)\n","    trained_image_embedding = trained_outputs.image_embeds\n","    trained_text_embedding = trained_outputs.text_embeds\n","\n","    # 기존 모델 임베딩\n","    original_outputs = original_model(input_ids=input_ids, pixel_values=pixel_values)\n","    original_image_embedding = original_outputs.image_embeds\n","    original_text_embedding = original_outputs.text_embeds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","trained_cos_sim = F.cosine_similarity(trained_image_embedding, trained_text_embedding)\n","original_cos_sim = F.cosine_similarity(original_image_embedding, original_text_embedding)\n","\n","print(f\"Trained Model Cosine Similarity: {trained_cos_sim.item()}\")\n","print(f\"Original Model Cosine Similarity: {original_cos_sim.item()}\")\n","\n","# 비교\n","if trained_cos_sim > original_cos_sim:\n","    print(\"The trained model has a higher cosine similarity.\")\n","else:\n","    print(\"The original model has a higher or equal cosine similarity.\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5970119,"sourceId":9751233,"sourceType":"datasetVersion"},{"datasetId":5975463,"sourceId":9758573,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":151301,"modelInstanceId":128426,"sourceId":151249,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
