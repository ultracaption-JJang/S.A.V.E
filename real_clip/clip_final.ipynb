{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-30T10:56:59.444091Z","iopub.status.busy":"2024-10-30T10:56:59.443754Z","iopub.status.idle":"2024-10-30T10:57:20.372547Z","shell.execute_reply":"2024-10-30T10:57:20.371546Z","shell.execute_reply.started":"2024-10-30T10:56:59.444057Z"},"trusted":true},"outputs":[],"source":["import os\n","import json\n","import torch\n","import wandb\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import CLIPProcessor, CLIPModel, Trainer, TrainingArguments\n","from PIL import Image\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# 1. Custom Dataset 준비 (JSON 파일에서 이미지와 텍스트 정보 로드)\n","class CustomCLIPDataset(Dataset):\n","    def __init__(self, json_path, image_dir, processor):\n","        with open(json_path, 'r') as f:\n","            self.data = json.load(f)  # JSON 파일 로드\n","        self.processor = processor\n","        self.image_dir = image_dir\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        image_id = str(item['image_id']).zfill(12)\n","        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n","        caption = item['caption']\n","        \n","        # 이미지 로드 및 전처리\n","        image = Image.open(image_path).convert(\"RGB\")\n","        inputs = self.processor(images=image, text=[caption], return_tensors=\"pt\", padding=True)\n","        \n","        return {\n","            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n","            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n","            \"pixel_values\": inputs[\"pixel_values\"].squeeze()\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    input_ids = [item['input_ids'].squeeze(0) for item in batch]\n","    pixel_values = [item['pixel_values'].squeeze(0) for item in batch]\n","    \n","    input_ids_padded = pad_sequence(input_ids, batch_first=True)\n","    pixel_values = torch.stack(pixel_values)  \n","    \n","    return {\n","        'input_ids': input_ids_padded,\n","        'pixel_values': pixel_values\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # 커스텀 Trainer 클래스 정의\n","# class CLIPTrainer(Trainer):\n","#     def compute_loss(self, model, inputs, return_outputs=False):\n","#         # 모델에서 출력 얻기\n","#         outputs = model(input_ids=inputs[\"input_ids\"], \n","#                         attention_mask=inputs[\"attention_mask\"], \n","#                         pixel_values=inputs[\"pixel_values\"])\n","        \n","#         # 텍스트 및 이미지 임베딩 추출\n","#         image_features = outputs.image_embeds\n","#         text_features = outputs.text_embeds\n","        \n","#         # Contrastive Loss 계산\n","#         loss_fn = ContrastiveLoss()\n","#         loss = loss_fn(image_features, text_features)\n","        \n","#         return (loss, outputs) if return_outputs else loss"]},{"cell_type":"markdown","metadata":{},"source":["## 캐글: Add-ons에서 본인 wandb키 입력하기 \n","## 로컬: 알아서 ㅎㅎ"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value = user_secrets.get_secret(\"wandbkey\")\n","os.environ[\"WANDB_API_KEY\"] = secret_value\n","\n","wandb.login()"]},{"cell_type":"markdown","metadata":{},"source":["## 파라미터 \n"," - 두 버전이 두 개가 있으니 config 파라미터 확인하고 train 함수 돌릴 것"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sweep_config = {\n","    'method': 'random',  # 하이퍼파라미터 검색 방법 ('grid', 'random', 'bayes' 중 선택)\n","    'metric': {\n","        'name': 'loss',  # 최적화할 메트릭 이름\n","        'goal': 'minimize'  # 목표: 'maximize' 또는 'minimize'\n","    },\n","    'parameters': {\n","        'learning_rate': {\n","            'distribution': 'uniform',  # 'uniform' 분포에서 값을 샘플링\n","            'min': 0.0001,\n","            'max': 0.001\n","        },\n","        'epochs': {\n","            'values': [10, 20, 30]  # 실험할 epoch 값 목록\n","        }\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_config = {\n","    'method': 'random',  # 하이퍼파라미터 검색 방법: 'random'\n","    'metric': {\n","        'name': 'loss',  # 최적화할 메트릭 이름\n","        'goal': 'minimize'  # 목표: 손실 값을 최소화\n","    },\n","    'parameters': {\n","        'layers': {\n","            'values': [3]  # 고정된 값: 3개 레이어\n","        },\n","        'layer_dimension': {\n","            'values': ['1024-128-3']  # 레이어 차원 설정\n","        },\n","        'epochs': {\n","            'values': [20]  # 고정된 에폭 수: 20\n","        },\n","        'adam_epsilon': {\n","            'values': [1e-8]  # 고정된 Adam epsilon 값\n","        },\n","        'adam_beta': {\n","            'values': [(0.9, 0.999)]  # 고정된 Adam 베타 값\n","        },\n","        'gradient_clipping': {\n","            'values': [2.0]  # 고정된 그래디언트 클리핑 값\n","        },\n","        'learning_rate': {\n","            'values': [1e-6, 3e-6, 5e-6]  # 학습률 후보 값\n","        },\n","        'batch_size': {\n","            'values': [16]  # 배치 크기 후보 값\n","        },\n","        'dropout': {\n","            'values': [0.0, 0.1, 0.4]  # 드롭아웃 확률 후보 값\n","        }\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_id = wandb.sweep(sweep_config, project=\"clip_experiment\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 데이터로더 부분 경로 바꿔줘야합니다"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_image_dir = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_train\"\n","val_image_dir = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_val\"\n","train_json_file = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_train_captions.json\"\n","val_json_file = \"/kaggle/input/1000-coco-final/realrealreal_coco_dataset/realreal_val_captions.json\"\n","\n","# 3. Initialize Dataset and DataLoader\n","train_dataset = CustomCLIPDataset(train_json_file, train_image_dir, processor)\n","val_dataset = CustomCLIPDataset(val_json_file, val_image_dir, processor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## RECALL_K"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_recall_at_k(logits, labels, k):\n","    top_k_predictions = torch.topk(logits, k=k, dim=1).indices\n","    correct = 0\n","    total = labels.size(0)\n","\n","    for i in range(total):\n","        if labels[i] in top_k_predictions[i]:\n","            correct += 1\n","\n","    recall_at_k = correct / total\n","    return recall_at_k"]},{"cell_type":"markdown","metadata":{},"source":["## TRAIN!!"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def calculate_recall_at_k(logits, labels, k):\n","    top_k_predictions = torch.topk(logits, k=k, dim=1).indices\n","    correct = 0\n","    total = labels.size(0)\n","\n","    for i in range(total):\n","        if labels[i] in top_k_predictions[i]:\n","            correct += 1\n","\n","    recall_at_k = correct / total\n","    return recall_at_k\n","\n","def train():\n","    wandb.init()  # Initialize W&B run\n","    batch_size = wandb.config.batch_size\n","    learning_rate = wandb.config.learning_rate\n","    epochs = wandb.config.epochs\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","    optimizer = Adam(model.parameters(), lr=learning_rate)\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    model.to(device)\n","\n","    best_train_accuracy = 0.0  # Initialize the best training accuracy\n","    best_val_accuracy = 0.0  # Initialize the best validation accuracy\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","\n","        for batch in train_dataloader:\n","            optimizer.zero_grad()\n","\n","            input_ids = batch['input_ids'].to(device)\n","            pixel_values = batch['pixel_values'].to(device)\n","            \n","            outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n","            logits_per_image = outputs.logits_per_image\n","            labels = torch.arange(logits_per_image.size(0)).to(logits_per_image.device)\n","\n","            # Compute loss\n","            loss = loss_fn(logits_per_image, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","            # Calculate training accuracy\n","            predictions = torch.argmax(logits_per_image, dim=1)\n","            correct_train += (predictions == labels).sum().item()\n","            total_train += labels.size(0)\n","\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        train_accuracy = correct_train / total_train\n","        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n","\n","        # Update best training accuracy if applicable\n","        if train_accuracy > best_train_accuracy:\n","            best_train_accuracy = train_accuracy\n","            print(f\"New best training accuracy: {best_train_accuracy}\")\n","\n","        # Validation Loop\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        val_total_loss = 0  # Initialize total validation loss\n","\n","        # Initialize Recall@k metrics\n","        recall_at_1 = 0.0\n","        recall_at_5 = 0.0\n","        recall_at_10 = 0.0\n","\n","        with torch.no_grad():\n","            for batch in val_dataloader:\n","                input_ids = batch['input_ids'].to(device)\n","                pixel_values = batch['pixel_values'].to(device)\n","\n","                outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n","                logits_per_image = outputs.logits_per_image\n","                labels = torch.arange(logits_per_image.size(0)).to(logits_per_image.device)\n","\n","                # Compute validation loss\n","                val_loss = loss_fn(logits_per_image, labels)\n","                val_total_loss += val_loss.item()\n","\n","                # Calculate validation accuracy\n","                predictions = torch.argmax(logits_per_image, dim=1)\n","                correct += (predictions == labels).sum().item()\n","                total += labels.size(0)\n","\n","                # Calculate Recall@k\n","                recall_at_1 += calculate_recall_at_k(logits_per_image, labels, k=1)\n","                recall_at_5 += calculate_recall_at_k(logits_per_image, labels, k=5)\n","                recall_at_10 += calculate_recall_at_k(logits_per_image, labels, k=10)\n","\n","        avg_val_loss = val_total_loss / len(val_dataloader)\n","        val_accuracy = correct / total\n","        recall_at_1 /= len(val_dataloader)\n","        recall_at_5 /= len(val_dataloader)\n","        recall_at_10 /= len(val_dataloader)\n","\n","        print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n","        print(f\"Recall@1: {recall_at_1}, Recall@5: {recall_at_5}, Recall@10: {recall_at_10}\")\n","\n","        # Log metrics to W&B\n","        wandb.log({\n","            \"epoch\": epoch, \n","            \"loss\": avg_train_loss, \n","            \"train_accuracy\": train_accuracy, \n","            \"val_loss\": avg_val_loss, \n","            \"val_accuracy\": val_accuracy,\n","            \"recall_at_1\": recall_at_1,\n","            \"recall_at_5\": recall_at_5,\n","            \"recall_at_10\": recall_at_10\n","        })\n","\n","        # Check if this is the best model so far\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            # Save the best model\n","            save_path = \"best_model.pth\"\n","            torch.save(model.state_dict(), save_path)\n","            print(f\"Best model saved with accuracy: {best_val_accuracy}\")\n","\n","            # Save model to W&B\n","            artifact = wandb.Artifact('best_model', type='model')\n","            artifact.add_file(save_path)\n","            wandb.log_artifact(artifact)\n","\n","# Start the W&B sweep agent\n","wandb.agent(sweep_id, train)"]},{"cell_type":"markdown","metadata":{},"source":["## 파라미터 더 추가"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def train():\n","    wandb.init()  # Initialize W&B run\n","    # Hyperparameters from wandb.config\n","    batch_size = wandb.config.batch_size\n","    learning_rate = wandb.config.learning_rate\n","    epochs = wandb.config.epochs\n","    adam_epsilon = wandb.config.adam_epsilon\n","    adam_beta = wandb.config.adam_beta\n","    gradient_clipping = wandb.config.gradient_clipping\n","    dropout = wandb.config.dropout\n","\n","    # Early Stopping 설정\n","    patience = 5  # 개선이 없을 때 몇 번의 epoch 동안 기다릴지\n","    no_improvement_count = 0  # 개선이 없는 epoch 수를 추적\n","    best_val_loss = float('inf')  # 가장 낮은 검증 손실 값을 추적\n","\n","    # Update your model here to use the dropout\n","    # Example: model = YourModel(dropout=dropout)\n","    \n","    # Model initialization with the configured dropout\n","    model.to(device)\n","\n","    # Dataloaders\n","    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","    # Optimizer\n","    optimizer = Adam(\n","        model.parameters(),\n","        lr=learning_rate,\n","        eps=adam_epsilon,\n","        betas=adam_beta\n","    )\n","    \n","    # Loss function\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","\n","    # Gradient clipping\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n","\n","    best_train_accuracy = 0.0  # Initialize the best training accuracy\n","    best_val_accuracy = 0.0  # Initialize the best validation accuracy\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","\n","        for batch in train_dataloader:\n","            optimizer.zero_grad()\n","\n","            input_ids = batch['input_ids'].to(device)\n","            pixel_values = batch['pixel_values'].to(device)\n","            \n","            outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n","            logits_per_image = outputs.logits_per_image\n","            labels = torch.arange(logits_per_image.size(0)).to(logits_per_image.device)\n","\n","            # Compute loss\n","            loss = loss_fn(logits_per_image, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","            # Calculate training accuracy\n","            predictions = torch.argmax(logits_per_image, dim=1)\n","            correct_train += (predictions == labels).sum().item()\n","            total_train += labels.size(0)\n","\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        train_accuracy = correct_train / total_train\n","        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n","\n","        # Update best training accuracy if applicable\n","        if train_accuracy > best_train_accuracy:\n","            best_train_accuracy = train_accuracy\n","            print(f\"New best training accuracy: {best_train_accuracy}\")\n","\n","        # Validation Loop\n","        model.eval()\n","        correct = 0\n","        total = 0\n","        val_total_loss = 0  # Initialize total validation loss\n","\n","        # Initialize Recall@k metrics\n","        recall_at_1 = 0.0\n","        recall_at_5 = 0.0\n","        recall_at_10 = 0.0\n","\n","        with torch.no_grad():\n","            for batch in val_dataloader:\n","                input_ids = batch['input_ids'].to(device)\n","                pixel_values = batch['pixel_values'].to(device)\n","\n","                outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n","                logits_per_image = outputs.logits_per_image\n","                labels = torch.arange(logits_per_image.size(0)).to(logits_per_image.device)\n","\n","                # Compute validation loss\n","                val_loss = loss_fn(logits_per_image, labels)\n","                val_total_loss += val_loss.item()\n","\n","                # Calculate validation accuracy\n","                predictions = torch.argmax(logits_per_image, dim=1)\n","                correct += (predictions == labels).sum().item()\n","                total += labels.size(0)\n","\n","                # Calculate Recall@k\n","                recall_at_1 += calculate_recall_at_k(logits_per_image, labels, k=1)\n","                recall_at_5 += calculate_recall_at_k(logits_per_image, labels, k=5)\n","                recall_at_10 += calculate_recall_at_k(logits_per_image, labels, k=10)\n","\n","        avg_val_loss = val_total_loss / len(val_dataloader)\n","        val_accuracy = correct / total\n","        recall_at_1 /= len(val_dataloader)\n","        recall_at_5 /= len(val_dataloader)\n","        recall_at_10 /= len(val_dataloader)\n","\n","        print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {val_accuracy}\")\n","        print(f\"Recall@1: {recall_at_1}, Recall@5: {recall_at_5}, Recall@10: {recall_at_10}\")\n","\n","        # Log metrics to W&B\n","        wandb.log({\n","            \"epoch\": epoch, \n","            \"loss\": avg_train_loss, \n","            \"train_accuracy\": train_accuracy, \n","            \"val_loss\": avg_val_loss, \n","            \"val_accuracy\": val_accuracy,\n","            \"recall_at_1\": recall_at_1,\n","            \"recall_at_5\": recall_at_5,\n","            \"recall_at_10\": recall_at_10\n","        })\n","\n","        # Early Stopping 체크\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            no_improvement_count = 0  # 개선되었으므로 카운트 리셋\n","        else:\n","            no_improvement_count += 1  # 개선되지 않으면 카운트 증가\n","\n","        if no_improvement_count >= patience:\n","            print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n","            break  # 학습 종료\n","\n","        # Check if this is the best model so far\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            # Save the best model\n","            save_path = \"best_model.pth\"\n","            torch.save(model.state_dict(), save_path)\n","            print(f\"Best model saved with accuracy: {best_val_accuracy}\")\n","\n","            # Save model to W&B\n","            artifact = wandb.Artifact('best_model', type='model')\n","            artifact.add_file(save_path)\n","            wandb.log_artifact(artifact)\n","\n","# Start the W&B sweep agent\n","wandb.agent(sweep_id, train)"]},{"cell_type":"markdown","metadata":{},"source":["# 아래 무시해도 됨!!!!!!!!!!!!!"]},{"cell_type":"markdown","metadata":{},"source":["## 코사인 유사도 비교"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trained_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","trained_model.load_state_dict(torch.load(\"best_model.pth\"))\n","trained_model.to(device)\n","trained_model.eval()\n","\n","original_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","original_model.to(device)\n","original_model.eval()\n","\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 원하는 텍스트 데이터 입력\n","- 1.\t“A black SUV parked next to a ‘Pay Here’ parking meter on a busy street.”\n","- 2.\t“Cars lined up along the street near a parking sign, with a view of distant hills.”\n","- 3.\t“A parking area with vehicles and a visible parking payment station in the foreground.”\n","- 4.\t“An urban street scene with parked cars and a ‘Pay Here’ sign for parking fees.”\n","- 5.\t“A black vehicle with a license plate parked beside a meter that says ‘Pay Here’.”"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_path = \"/Users/psjj/Downloads/coco2017/realrealreal_coco_dataset/realreal_test/000000092212.jpg\"  # 테스트할 이미지 경로\n","text = \"Your descriptive text here\" \n","\n","image = Image.open(image_path).convert(\"RGB\")\n","inputs = processor(images=image, text=[text], return_tensors=\"pt\", padding=True)\n","pixel_values = inputs[\"pixel_values\"].to(device)\n","input_ids = inputs[\"input_ids\"].to(device)\n","attention_mask = inputs[\"attention_mask\"].to(device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    # 학습된 모델 임베딩\n","    trained_outputs = trained_model(input_ids=input_ids, pixel_values=pixel_values)\n","    trained_image_embedding = trained_outputs.image_embeds\n","    trained_text_embedding = trained_outputs.text_embeds\n","\n","    # 기존 모델 임베딩\n","    original_outputs = original_model(input_ids=input_ids, pixel_values=pixel_values)\n","    original_image_embedding = original_outputs.image_embeds\n","    original_text_embedding = original_outputs.text_embeds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","trained_cos_sim = F.cosine_similarity(trained_image_embedding, trained_text_embedding)\n","original_cos_sim = F.cosine_similarity(original_image_embedding, original_text_embedding)\n","\n","print(f\"Trained Model Cosine Similarity: {trained_cos_sim.item()}\")\n","print(f\"Original Model Cosine Similarity: {original_cos_sim.item()}\")\n","\n","# 비교\n","if trained_cos_sim > original_cos_sim:\n","    print(\"The trained model has a higher cosine similarity.\")\n","else:\n","    print(\"The original model has a higher or equal cosine similarity.\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5970119,"sourceId":9751233,"sourceType":"datasetVersion"},{"datasetId":5975463,"sourceId":9758573,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":151301,"modelInstanceId":128426,"sourceId":151249,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
