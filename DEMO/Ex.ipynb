{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a0e0b8-fb1f-4437-9815-3899575173d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: A close-up shot of food on the table.\n",
      "\n",
      "\"I don't know what's going on,\" he said. \"I don't know what's going on. I don't know what's going on. I don't know what\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import requests\n",
    "\n",
    "# CLIP 모델 및 프로세서 로드\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# GPT-2 모델 및 토크나이저 로드\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_image(image_path=None, url=None):\n",
    "    if url:\n",
    "        # URL에서 이미지 로드\n",
    "        image = Image.open(requests.get(url, stream=True).raw)\n",
    "    elif image_path:\n",
    "        # 로컬 경로에서 이미지 로드\n",
    "        image = Image.open(image_path)\n",
    "    else:\n",
    "        raise ValueError(\"Image path or URL must be provided.\")\n",
    "    return image\n",
    "\n",
    "# 이미지에서 특징 추출\n",
    "def extract_image_features(image_path=None, url=None):\n",
    "    image = load_image(image_path=image_path, url=url)\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    return image_features\n",
    "\n",
    "# 이미지 특징과 가장 유사한 텍스트 생성\n",
    "def find_similar_text(image_features, candidate_texts):\n",
    "    inputs = clip_processor(text=candidate_texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**inputs)\n",
    "\n",
    "    # 이미지 특징과 텍스트 특징 간의 유사도를 계산\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = torch.matmul(image_features, text_features.T)\n",
    "\n",
    "    # 가장 유사한 텍스트를 선택\n",
    "    best_match_index = similarity.argmax().item()\n",
    "    return candidate_texts[best_match_index]\n",
    "\n",
    "# 이미지 특징을 기반으로 캡션 생성\n",
    "def generate_caption_from_image(image_path=None, url=None):\n",
    "    # 이미지에서 특징 추출\n",
    "    image_features = extract_image_features(image_path=image_path, url=url)\n",
    "\n",
    "    \n",
    "    \n",
    "    # CLIP 모델로부터 텍스트 후보군 생성 (캡션 후보)\n",
    "    candidate_texts = [\n",
    "        \"A photo of a cat\",\n",
    "        \"A picture of a dog\",\n",
    "        \"A landscape with mountains\",\n",
    "        \"A person riding a horse\",\n",
    "        \"An aerial view of a city\",\n",
    "        \"A close-up shot of food\",\n",
    "    ]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 이미지 특징과 가장 유사한 텍스트 찾기\n",
    "    best_caption = find_similar_text(image_features, candidate_texts)\n",
    "\n",
    "    # GPT-2 모델을 사용하여 이미지 특징 기반으로 캡션 확장\n",
    "    input_ids = gpt2_tokenizer.encode(best_caption, return_tensors='pt')\n",
    "    \n",
    "    #attention_mask = input_ids.ne(gpt2_tokenizer.pad_token_id).long()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # outputs = gpt2_model.generate(input_ids, max_length=30, num_return_sequences=1,\n",
    "        #                                num_beams=5, early_stopping=True)\n",
    "        outputs = gpt2_model.generate(input_ids,\n",
    "                                      pad_token_id = gpt2_tokenizer.eos_token_id,\n",
    "                                      max_length=50,\n",
    "                                      num_return_sequences=1,\n",
    "                                       num_beams=5,\n",
    "                                       early_stopping=True)\n",
    "\n",
    "    generated_caption = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_caption\n",
    "\n",
    "# 예시 이미지 경로 또는 URL\n",
    "image_path = \"/Users/psjj/Downloads/coco2017/train2017/000000000009.jpg\"  # 로컬 이미지 경로\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # URL 이미지\n",
    "\n",
    "# 캡션 생성\n",
    "generated_caption = generate_caption_from_image(image_path=image_path)  # 또는 image_path=image_path\n",
    "# generated_caption = generated_caption.split('.')[0] + '.' # 다 안될 경우 이 코드 추가\n",
    "print(\"Generated Caption:\", generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de2684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: This image is about to go viral on social media.\n",
      "\n",
      "It's a picture of a man with a gun in his hand.\n",
      "\n",
      "It's a picture of a man with a gun in his hand.\n",
      "\n",
      "It's a picture of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# CLIP 및 GPT-2 모델 로드\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPT-2의 패딩 토큰 ID 설정\n",
    "gpt2_model.config.pad_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "# 매핑 네트워크 정의 (CLIP 이미지 특징 -> GPT-2 입력으로 변환)\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, clip_embedding_dim, gpt_embedding_dim):\n",
    "        super(MappingNetwork, self).__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            nn.Linear(clip_embedding_dim, gpt_embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gpt_embedding_dim, gpt_embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, clip_features):\n",
    "        return self.mapping(clip_features)\n",
    "\n",
    "# 매핑 네트워크 초기화\n",
    "clip_embedding_dim = 512  # CLIP 이미지 특징 차원\n",
    "gpt_embedding_dim = gpt2_model.transformer.wte.weight.size(1)  # GPT-2 임베딩 차원\n",
    "mapping_network = MappingNetwork(clip_embedding_dim, gpt_embedding_dim)\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_image(image_path=None, url=None):\n",
    "    if url:\n",
    "        image = Image.open(requests.get(url, stream=True).raw)\n",
    "    elif image_path:\n",
    "        image = Image.open(image_path)\n",
    "    else:\n",
    "        raise ValueError(\"Image path or URL must be provided.\")\n",
    "    return image\n",
    "\n",
    "# 이미지 특징 추출\n",
    "def extract_image_features(image_path=None, url=None):\n",
    "    image = load_image(image_path=image_path, url=url)\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "# 이미지 특징을 기반으로 캡션 생성\n",
    "def generate_caption_from_image(image_path=None, url=None):\n",
    "    # 이미지 특징 추출\n",
    "    image_features = extract_image_features(image_path=image_path, url=url)\n",
    "\n",
    "    # CLIP 이미지 특징을 매핑 네트워크를 통해 GPT-2 임베딩으로 변환\n",
    "    mapped_features = mapping_network(image_features).unsqueeze(0)\n",
    "\n",
    "    # GPT-2에 프롬프트 설정\n",
    "    input_ids = gpt2_tokenizer.encode(\"This image is about\", return_tensors=\"pt\")\n",
    "\n",
    "    # GPT-2 임베딩과 매핑된 CLIP 특징을 결합\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "    generated_caption = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_caption\n",
    "\n",
    "# 이미지 경로 또는 URL 설정\n",
    "image_path = \"/Users/psjj/Downloads/coco2017/train2017/000000000009.jpg\"  # 로컬 이미지 경로\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # COCO 데이터셋 이미지\n",
    "\n",
    "# 캡션 생성\n",
    "generated_caption = generate_caption_from_image(image_path=image_path)  # 또는 url=url\n",
    "print(\"Generated Caption:\", generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1154e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import requests\n",
    "\n",
    "# CLIP 모델 및 프로세서 로드\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# GPT-2 모델 및 토크나이저 로드\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_image(image_path=None, url=None):\n",
    "    if url:\n",
    "        # URL에서 이미지 로드\n",
    "        image = Image.open(requests.get(url, stream=True).raw)\n",
    "    elif image_path:\n",
    "        # 로컬 경로에서 이미지 로드\n",
    "        image = Image.open(image_path)\n",
    "    else:\n",
    "        raise ValueError(\"Image path or URL must be provided.\")\n",
    "    return image\n",
    "\n",
    "# 이미지에서 특징 추출\n",
    "def extract_image_features(image_path=None, url=None):\n",
    "    image = load_image(image_path=image_path, url=url)\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    return image_features\n",
    "\n",
    "# 이미지 특징과 가장 유사한 텍스트 생성\n",
    "def find_similar_text(image_features, candidate_texts):\n",
    "    inputs = clip_processor(text=candidate_texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**inputs)\n",
    "\n",
    "    # 이미지 특징과 텍스트 특징 간의 유사도를 계산\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = torch.matmul(image_features, text_features.T)\n",
    "\n",
    "    # 가장 유사한 텍스트를 선택\n",
    "    best_match_index = similarity.argmax().item()\n",
    "    return candidate_texts[best_match_index]\n",
    "\n",
    "# 이미지 특징을 기반으로 캡션 생성\n",
    "def generate_caption_from_image(image_path=None, url=None):\n",
    "    # 이미지에서 특징 추출\n",
    "    image_features = extract_image_features(image_path=image_path, url=url)\n",
    "\n",
    "    # GPT-2 모델에 프롬프트 설정 (프롬프트를 이미지와 관련된 문구로 생성)\n",
    "    prompt = \"This is an image of\"\n",
    "\n",
    "    # GPT-2에 프롬프트를 입력\n",
    "    input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(input_ids,\n",
    "                                      pad_token_id=gpt2_tokenizer.eos_token_id,\n",
    "                                      max_length=50,\n",
    "                                      num_return_sequences=1,\n",
    "                                      num_beams=5,\n",
    "                                      early_stopping=True)\n",
    "\n",
    "    generated_caption = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_caption\n",
    "\n",
    "# 예시 이미지 경로 또는 URL\n",
    "image_path = \"/Users/psjj/Downloads/coco2017/train2017/000000000009.jpg\"  # 로컬 이미지 경로\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # URL 이미지\n",
    "\n",
    "# 캡션 생성\n",
    "generated_caption = generate_caption_from_image(image_path=image_path)  # 또는 image_path=image_path\n",
    "# generated_caption = generated_caption.split('.')[0] + '.' # 다 안될 경우 이 코드 추가\n",
    "print(\"Generated Caption:\", generated_caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
