{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0.package install"
      ],
      "metadata": {
        "id": "qKeKaGX1ejJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers pillow"
      ],
      "metadata": {
        "id": "f_cGi4dYkWVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. captioning generation\n",
        "1) CLIP으로 이미지에서 특징을 추출합니다.  \n",
        "2) 추출된 이미지 특징을 GPT-2 모델에 입력합니다.  \n",
        "3) 생성된 텍스트를 확인합니다.  \n",
        "\n",
        "\n",
        "---\n",
        "앞으로 구현할 것  \n",
        "4) TTS 모델을 통해 생성된 텍스트를 음성으로 변환해줍니다.\n"
      ],
      "metadata": {
        "id": "qGdk316KmFrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import requests\n",
        "\n",
        "# CLIP 모델 및 프로세서 로드\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "# GPT-2 모델 및 토크나이저 로드\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 이미지 로드 및 전처리\n",
        "def load_image(image_path):\n",
        "    # 이미지 로드 : 원하는 이미지로 바꿀 수 있어요\n",
        "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "    image = Image. open(requests.get(url, stream=True).raw)\n",
        "    image\n",
        "    # image = Image.open(image_path)\n",
        "    return image\n",
        "\n",
        "# 이미지에서 특징 추출\n",
        "def extract_image_features(image_path):\n",
        "    image = load_image(image_path)\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_image_features(**inputs)\n",
        "    return features\n",
        "\n",
        "# 이미지 특징을 기반으로 캡션 생성\n",
        "def generate_caption_from_image(image_path):\n",
        "    # 이미지에서 특징 추출\n",
        "    image_features = extract_image_features(image_path)\n",
        "\n",
        "    # GPT-2 모델을 사용하여 캡션 생성\n",
        "    # 특징을 텍스트로 변환하기 위해 랜덤한 시작 문장을 사용합니다.\n",
        "    input_ids = gpt2_tokenizer.encode(\"A photo of\", return_tensors='pt')\n",
        "\n",
        "    # GPT-2 모델로 캡션 생성\n",
        "    with torch.no_grad():\n",
        "        outputs = gpt2_model.generate(input_ids, max_length=30, num_return_sequences=1,\n",
        "                                       num_beams=5, early_stopping=True)\n",
        "\n",
        "    generated_caption = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_caption\n",
        "\n",
        "# 예시 이미지 경로\n",
        "image_path = \"path/to/your/image.jpg\"  # 사용할 이미지 경로로 수정\n",
        "\n",
        "# 캡션 생성\n",
        "generated_caption = generate_caption_from_image(image_path)\n",
        "print(\"Generated Caption:\", generated_caption)\n"
      ],
      "metadata": {
        "id": "m08fnKWmkbbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image. open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "i-B1eVAMldiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eE77MUGilgSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}